{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTACIÓN DE LAS LIBRERÍAS NECESARIAS PARA EJECUTAR NUESTRO CÓDIGO\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "### DEFINICIÓN DE LA FUNCIÓN MEDIANTE LA CUAL SE REALIZARÁ EL WEB SCRAPING\n",
    "\n",
    "def meteoPozuelo(url,mes_min,mes_max):\n",
    "    \n",
    "    mes_min_date   = datetime.strptime(mes_min,\"%m/%Y\")  ##Mayor o igual a diciembre de 2011\n",
    "\n",
    "    mes_max_date   = datetime.strptime(mes_max,\"%m/%Y\")  ##Menor o igual al mes en curso (abril de 2021)\n",
    "\n",
    "    dd = [] #Se crea una lista vacía donde se recopilará la información que se extrae de la web\n",
    "\n",
    "    campos = [\"Dia\",\"Mes\",\"Año\",\"Temperatura media\",\"Humedad media\",\"Rocío medio\",\"Presión media\",\"Velocidad media\",\n",
    "      \"Racha media\",\"Dirección media\",\"Precipitación mensual\",\"Precipitación anual\",\"Intensidad máxima por minuto\",\n",
    "      \"Temperatura máxima\",\"Temperatura mínima\",\"Humedad máxima\",\"Humedad mínima\",\"Presión máxima\",\"Presión mínima\",\n",
    "      \"Velocidad media máxima\",\"Racha máxima\"] #lista con los nombres de los campos a recopilar\n",
    "\n",
    "    dd.append(campos) #el primer elemento de dd son los nombres de los campos a recopilar\n",
    "    \n",
    "    while mes_min_date <= mes_max_date:\n",
    "\n",
    "        mes_min_YYYYMM = str(mes_min_date.year) + mes_min_date.strftime('%m') \n",
    "\n",
    "        url_complet = url + mes_min_YYYYMM\n",
    "\n",
    "        page = requests.get(url_complet)\n",
    "\n",
    "        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "        datos = soup.find_all('table')\n",
    "\n",
    "        datos = datos[:-3] #Eliminamos los tres últimos valores de la lista \"datos\" porque son tres tablas de datos\n",
    "                           #globales de todo el mes de temperatura y precipitaciones que no nos interesan en el estudio\n",
    "\n",
    "        for i in range(len(datos)):   #recorremos cada uno de los días del mes en curso\n",
    "\n",
    "            dia         = str(datos[i].find_all('tr')[0].td.contents).split()[0][2:]\n",
    "            mes         = str(datos[i].find_all('tr')[0].td.contents).split()[1]\n",
    "            año         = mes_min_YYYYMM[:4]\n",
    "            temp_avg    = str(datos[i].find_all('tr')[1].find_all('td')[1])[5:-5]\n",
    "            hum_avg     = str(datos[i].find_all('tr')[2].find_all('td')[1])[5:-5]\n",
    "            roci_avg    = str(datos[i].find_all('tr')[3].find_all('td')[1])[5:-5]\n",
    "            prec_avg    = str(datos[i].find_all('tr')[4].find_all('td')[1])[5:-5]\n",
    "            vel_avg     = str(datos[i].find_all('tr')[5].find_all('td')[1])[5:-5]\n",
    "            rach_avg    = str(datos[i].find_all('tr')[6].find_all('td')[1])[5:-5]\n",
    "            dir_avg     = str(datos[i].find_all('tr')[7].find_all('td')[1])[5:-5]\n",
    "            prec_men    = str(datos[i].find_all('tr')[8].find_all('td')[1])[5:-5]\n",
    "            prec_anu    = str(datos[i].find_all('tr')[9].find_all('td')[1])[5:-5]\n",
    "            int_max     = str(datos[i].find_all('tr')[10].find_all('td')[1])[5:-5]\n",
    "            temp_max    = str(datos[i].find_all('tr')[11].find_all('td')[1])[5:-5]\n",
    "            temp_min    = str(datos[i].find_all('tr')[12].find_all('td')[1])[5:-5]\n",
    "            hum_max     = str(datos[i].find_all('tr')[13].find_all('td')[1])[5:-5]\n",
    "            hum_min     = str(datos[i].find_all('tr')[14].find_all('td')[1])[5:-5]\n",
    "            prec_max    = str(datos[i].find_all('tr')[15].find_all('td')[1])[5:-5]\n",
    "            prec_min    = str(datos[i].find_all('tr')[16].find_all('td')[1])[5:-5]\n",
    "            vel_med_max = str(datos[i].find_all('tr')[17].find_all('td')[1])[5:-5]\n",
    "            rach_max    = str(datos[i].find_all('tr')[18].find_all('td')[1])[5:-5]\n",
    "            \n",
    "            dd_diaX = [dia,mes,año,temp_avg,hum_avg,roci_avg,prec_avg,vel_avg,rach_avg,dir_avg,prec_men,prec_anu,\n",
    "                           int_max,temp_max,temp_min,hum_max,hum_min,prec_max,prec_min,vel_med_max,rach_max]\n",
    "\n",
    "            dd.append(dd_diaX) \n",
    "\n",
    "        mes_min_date = mes_min_date + relativedelta(months = 1)\n",
    "        \n",
    "    return dd\n",
    "\n",
    "###A PARTIR DE ESTE PUNTO SE EJECUTARÁ LA FUNCIÓN Y SE CREARÁ EL FICHERO CSV QUE CONTENDRÁ EL DATASET EXTRAÍDO MEDIANTE WEB SCRAPING\n",
    "\n",
    "url = \"http://www.meteopozuelo.es/wxhistory.php?date=\"  #URL sin el mes a cargar\n",
    "\n",
    "mes_min = '01/2018' #Mes mínimo a cargar en formato MM/YYYY\n",
    "\n",
    "mes_max = '02/2021' #Mes máximo a cargar en formato MM/YYYY\n",
    "\n",
    "dataset = meteoPozuelo(url,mes_min,mes_max)\n",
    "\n",
    "directorio_actual = os.getcwd()\n",
    "\n",
    "nombre_archivo = \"datos_meteorologicos_pozuelo_dataset.csv\"\n",
    "\n",
    "ruta_archivo = os.path.join(directorio_actual,nombre_archivo)\n",
    "\n",
    "with open(ruta_archivo,'w',newline='') as archivo_csv:\n",
    "    writer = csv.writer(archivo_csv, delimiter=';')\n",
    "    for i in dataset:\n",
    "        writer.writerow(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
